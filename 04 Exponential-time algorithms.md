## ETH and SETH

### Explain the statement of the exponential-time hypothesis ETH and the strong exponential-time hypothesis SETH. Consider the following:

The Exponential Time Hypothesis (ETH) states that 3-SAT cannot be solved in subexponential time, specifically that it requires at least $2^{cn}$ time for some constant $c > 0,$ where $n$ is the number of variables, ie $2^{o(cn)}$, small O if n 

SETH stands for the strong exponential time hypothesis and states that there is a constantÂ $k$Â such that no algorithm can solveÂ k-SAT onÂ $n$Â variables with clauses of width at mostÂ $k$Â in timeÂ $2^{(1âˆ’\epsilon)n}.$

Informally this means that the time required to solveÂ k-SAT increases exponentially with the number of variables.

**ETH**:  $\exists S >0$ such that 3-sat requires time $\geq 2^{sn}$


**Seth** : $\forall s >0,$ $\exists k$ s.t k-sat requires $2^{(1-s)n}$ time, k>2


#### Why do they imply $P \neq NP$?
ETH implies $P \neq NP$ because if $P = NP$, all NP problems, including 3-SAT, would be solvable in polynomial time, contradicting ETH.
#### Is an algorithm with running time $O(1.0001^{n})$ for 3-SAT possible under ETH? 
No, it is not, 
It grows slower than $2^{o(n)})$


#### Is an algorithm with running time $O(1001^{{n}/{\log n}})$ for 3-SAT possible under ETH?
mathrule $(a^m)^{n}=a^{mn}$
No. We can simplify this to: $$ 1001^{n/\log n} = 2^{\log_2(1001) \cdot (n/\log(n))} = 2^{O(n/\log(n))} $$
This is subexponential time sinceÂ $O(n/logâ¡(n))=o(n)$Â and thusÂ $2^{O(n/logâ¡(n)}=2^{o(n)}$. ETH rules out subexponential time algorithms for 3-SAT, so this is not possible.
#### Are either of these algorithms possible under SETH?

Since seth is stronger then these runtimes are not allowed 

#### Briefly give two examples for concrete super-polynomial running times for 3-SAT that are ruled out under ETH.
1. $O(2^{n/100})$Â this is super-polynomial but sub-exponential time complexity.
2. $O(1.999^n)$Â this is super-polynomial but sub-exponential time complexity.
### What is the sparsification lemma, and why is it useful?
The sparsification lemma states that any CNF formula $\phi$ with $n$ variables and $m$ clauses can be written as a disjunction of $O(2^{\epsilon n})$ CNF formulas, each with $O(n / \epsilon)$ clauses, for any $\epsilon > 0$

In simpler terms, the lemma allows us to "sparsify" a dense kk-SAT formula into a collection of smaller, sparse formulas without significantly increasing the overall complexity of the problem.

This should all take $2^{\epsilon n} \cdot \text{poly}(n)$ time.
### Describe the Orthogonal Vectors problem.
Given two setsÂ LÂ andÂ RÂ ofÂ nÂ size vectors fromÂ $\{0,1\}^d$Â withÂ $d=poly \log(n$), the problem is to determine if there is a pair of vectorsÂ $uâˆˆL$Â andÂ $vâˆˆR$Â such thatÂ $uâ‹…v=0$. This generally takesÂ $O(n^2)$Â time to chech
![](pics/TheOrthogonalVectors.png)
#### Which lower bound does SETH yield for this problem? Sketch the lower bound.
![](pics/SethImpliesOV.png)
We can then reduce the problem to diameter. Diameter in a connected undirected graph is the smallest positive integerÂ kÂ such that every vertex has a path to each other vertex of length at mostÂ k
![](pics/diameter.png)

![](pics/Hardness%20of%20computing%20the%20diameter.png)
![](pics/Reduction%20from%20OV%20to%20Diameter.png)

#### Is an $O(n^{2}/\log n)$ algorithm or an $O(n^{2} / \sqrt{n})$ time algorithm possible under SETH?

**$O(n^2/log_{2}n)$**

no, since n^2 is smaller than seth

**$O(n^2/n^{1/2})$**



no since no are polynomials run times 
### Describe the k-Dominating Set problem. 
A dominating set in an undirected graph is a subset of the vertices such that every vertex is either connected to a vertex in S or is in S itself. A k-dominating set, is a dominating set of size exactly k.
![](pics/dominatingSet.png)
![](pics/SETH-hardness%20of%20k-dominating%20set.png)
![](pics/SETH-hardness%20of%20k-dominating%20set1.png)
#### Which lower bound does SETH yield for this problem? Sketch the lower bound

## Inclusion/Exclusion

### State and explain the inclusion-exclusion principle.
![](pics/inclusion2.png)![](pics/inclusion3.png)
Essentially we add the sizes of the sets, then subtract the sizes of the intersections of the sets, then add the size of the intersection of all sets, this is because we've removed that size twice.

The sum hasÂ $2^{nâˆ’1}$Â terms, so it's not practical to compute by hand.
### Describe how to solve the Hamiltonian cycle problem using inclusion-exclusion.
The Hamiltonian cycle problem is the problem of determining whether a given graph contains a Hamiltonian cycle. 
A Hamiltonian cycle is a cycle that visits every vertex exactly once. This problem is NP-hard.
#### Describe the role of walks and paths.
A walk is the loosest of the two, it is just a sequence of vertices. It can be open or closed, closed then we have that $V_{1}= V_{k+1}$
A path is a walk, but the sequence has distinct vertices. 

A closed n-walk is a Hamiltonian cycle if all vertices except the first and the last are unique. Removing the last vertex would make it a n-path
#### How can you count walks in polynomial time?
First define the graph as anÂ $nÃ—n$ adjacency matrixÂ $A$Â whereÂ $A_{ij}=1$Â if there is an edge betweenÂ iÂ andÂ j, andÂ 0Â otherwise.

![](pics/Counting_walks_n_2.png)

calculateÂ $A^k$Â then the entry atÂ $u,u$Â equals the number of closedÂ k-walks starting and ending inÂ $u$. Or open walks from between $v,u$

The trace of a matrix is the sum of its diagonal entries. Hence, $ğ‘¡ğ‘Ÿ(ğ´^n)$ computes a weighted sum of the number of closed n-walks.

 The Hamiltonian cycles are counted 2ğ‘› times each, once per vertex and direction.

Using matrix multiplication and fast exponentiation we can calculate this in polynomial time.
#### Why do you need to count them, and why is deciding existence not enough?
The inclusion-exclusion principle requires us to count the number of structures (e.g., closed walks) that satisfy certain conditions. By counting walks, we can systematically account for all possible ways to form a Hamiltonian cycle.

The inclusion-exclusion principle relies on exact counts to compute the number of valid Hamiltonian cycles. 

![](pics/counting_hamiltonian_cycles.png)
![](pics/hamiltonian_correctness.png)

We can simplify the description of the algorithm to:

1. Count all closed walks of lengthÂ nÂ in the graph.
2. Subtract the number of closed walks that miss at least one vertex.
3. Add the number of closed walks that miss at least two vertices, to correct over subtraction.
4. Continue this alternating inclusion-exclusion process until all cases are accounted for
We can say thatÂ $X$Â is the number of vertices that are not excluded in the walk.
#### Briefly compare this approach to the Bellman-Held-Karp dynamic programming algorithm for Hamiltonian cycles. Also outline the DP algorithm.
==TODO understand==
The Bellman-Held-Karp dynamic programming algorithm is an algorithm for computing the Hamiltonian cycle in a graph. It works as follows:

- For each subsetÂ $S$Â of vertices and a vertexÂ $vâˆˆS$, letÂ d$p[S][v]$Â represent the number of paths that start at a fixed vertex, visit all vertices inÂ $S$Â exactly once and end atÂ $v$.
- The base case isÂ $dp[v][v]=1$Â for the starting vertexÂ $v$.
- For each subsetÂ SÂ and vertexÂ v, computeÂ $dp[S][v]$Â by iterating over all predecessorsÂ $u$Â ofÂ $v$Â inÂ S: $$ dp[S][v] = \sum_{u\in S, u\neq v}dp[S\setminus {v}][u] \cdot A_{uv} $$
- Finally the number of Hamiltonian cycles is the sum ofÂ $dp[V][v]$Â for allÂ $v$Â adjacent to the starting vertex.

The dynamic programming algorithm is a bottom-up approach that builds up the solution incrementally by considering all possible paths and vertices. It is a more direct and efficient way to compute the Hamiltonian cycle compared to the inclusion-exclusion method, which relies on the principle of inclusion-exclusion to count the number of Hamiltonian cycles.

The time complexity isÂ $O(n^{2} \cdot 2^{n}$). This is essentially combinatorial vs implementation.

### Describe how to count perfect matchings in a graph using inclusion-exclusion. Which structures are counted in the individual terms of the inclusion-exclusion formula?
A perfect matching in a graph is a set of edges such that every vertex is incident to exactly one edge.

Finding a perfect matching is a polynomial time problem, but counting them is NP-hard.

A perfect matching in an ğ‘›-vertex undirected graph ğº = ğ‘‰, ğ¸ is a set of $ğ‘›/2$-vertex-disjoint edges.

We can solve this problem with set partitioning
![](pics/Perfect_matchings_setpartion.png)

inclusionâ€“exclusion algorithm for Set Partition with equal sized sets
From set partioning we have that we can count the number of perfect matchings in a graphÂ $G=(V,E)$Â as:

- Let the familyÂ $F$Â the the edgesÂ $E$Â of the graph and setÂ $k=n/2$.
- LetÂ $t(X)=|f:fâˆˆF,fâŠ†X|Â \text{ for }Â XâŠ†V.$
- Then the number of perfect matchings is: $$ \sum_{X\subseteq U}(-1)^{n-|X|}t(X)^{n/2} $$
This counts the perfect-matchings with multiplicityÂ $n/2$Â and runs inÂ $O(2^n|E|n)$Â time withÂ $O(nlogâ¡(|E|))$Â space.

The vanilla inclusionâ€“exclusion method 
We get a $2^{n}poly (n)$ time and polynomial space algorithm for counting the perfect matchings in a general ğ‘›-vertex graph
##### But we can do better
Number of edges in induces subgraphs
==TODO yikes==
![](pics/perfMatchSuggrah_1.png)
![](pics/perfMatchSuggrah_2.png)
![](pics/perfMatchSuggrah_3.png)
![](pics/perfMatchSuggrah_4.png)
#### How can you speed this up on bipartite graphs, and what are the structures counted there?
![](pics/permanent_of_square_matrix.png)
The permanent of aÂ 0/1Â square matrix is the number of perfect matchings in a bipartite graph. The permanent can by Ryser's algorithm be computes as: $$ per(A)=\sum_{X\subseteq[n]}(-1)^{n-|X|}\prod_{i=1}^n\left(\sum_{j\in X}A_{ij}\right) $$

The permanent differs from the determinant by not accounting for the sign of the permutation.

Calculating the permanent takesÂ $O(2^{n}n^{2})$Â time butÂ $n$Â is now only half the size of the original graph.
### Consider an n-vertex graph G.
#### How is the chromatic number of G defined?
![](pics/chromatic.png)
- A graph with chromatic number 1 has no edges as all vertices can be colored the same.
- A graph with chromatic number 2 is bipartite.
- A graph with chromatic numberÂ nÂ is complete.
#### How can you compute the chromatic $O(3^{n})$ number in time? The approach weâ€™ve studied reduces the problem to counting certain structures. What are these structures and how fast can you count them?


- Every colorclass is an independent set in the graph. 
- Let the ground set ğ‘ˆ be the set of vertices ğ‘‰. 
- Let the family â„± be the independent sets in the graph. 
- To see if ğ‘˜ colors are enough, we want to see if ğ‘ˆ can be covered by ğ‘˜ independent sets. 
- The smallest such ğ‘˜ is the chromatic number
We can do the calculation via induced subgraphs. First defineÂ $t(X)$Â for a subsetÂ $XâŠ†V$Â as the number of independent sets in the induced graphÂ $G[X]$, then:

![](pics/chromaticalog.png)